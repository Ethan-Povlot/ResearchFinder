{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "from io import BytesIO\n",
    "import docx\n",
    "from PyPDF2 import PdfReader\n",
    "from string import digits\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import keyboard\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "uni_names = list(set(pd.read_csv('uni_lst.csv')['University'].values.tolist()))\n",
    "\n",
    "def read_word_file(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    first_page_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        first_page_text.append(para.text)\n",
    "    return '\\n'.join(first_page_text)\n",
    "def download_wait(path_to_downloads=r'c:\\Users\\ethan\\Downloads'):#mostly deprecated\n",
    "    seconds = 0\n",
    "    dl_wait = True\n",
    "    while dl_wait and seconds < 60:\n",
    "        sleep(1)\n",
    "        dl_wait = False\n",
    "        for fname in os.listdir(path_to_downloads):\n",
    "            if fname.endswith('.crdownload'):\n",
    "                dl_wait = True\n",
    "        seconds += 1\n",
    "    return seconds\n",
    "def download_arxiv_old(arxiv_id):#mostly deprecated, just a backup method\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(f'https://arxiv.org/pdf/{arxiv_id}.pdf')\n",
    "    keyboard.press_and_release('ctrl + s')\n",
    "    sleep(1)\n",
    "    keyboard.write('temp.pdf')\n",
    "    keyboard.press_and_release('enter')\n",
    "    download_wait()\n",
    "    driver.quit()\n",
    "    return get_page(r\"C:\\Users\\ethan\\Downloads\\temp.pdf\")\n",
    "\n",
    "def download_arxiv(arxiv_id):\n",
    "    response = requests.get(f'https://arxiv.org/pdf/{arxiv_id}.pdf')\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        pdf_content = BytesIO(response.content)\n",
    "        #pdf_reader = PdfReader(pdf_content)\n",
    "        return get_page(pdf_content,reader=True)\n",
    "        \n",
    "    else:\n",
    "        return download_arxiv_old(arxiv_id)\n",
    "def osf_download_old(publication_id):\n",
    "    with open(\"temp.pdf\", \"wb\") as pdf_file:\n",
    "        pdf_file.write(requests.get(r'https://osf.io/download/'+publication_id).content)\n",
    "    return get_page('temp.pdf')\n",
    "\n",
    "def osf_download(publication_id):\n",
    "    publication_link = r'https://osf.io/download/'+publication_id\n",
    "    out=  get_page(BytesIO(requests.get(publication_link).content))\n",
    "    if out ==[]:\n",
    "        print('old')\n",
    "        return osf_download_old(publication_link)\n",
    "    return out\n",
    "\n",
    "def get_page(file_name, reader=False):\n",
    "    page = None\n",
    "    try:\n",
    "        reader  = PdfReader(file_name)#this should work either way as this function can take in either path or file\n",
    "        page = reader.pages[0].extract_text()\n",
    "    except:\n",
    "        try:\n",
    "            page = read_word_file(file_name)\n",
    "        except:\n",
    "            pass\n",
    "    if not reader:\n",
    "        os.remove(file_name)\n",
    "    if page == None:\n",
    "        return []\n",
    "    return get_affiliations_and_emails(page)\n",
    "def get_affiliations_and_emails(page):\n",
    "    page = page.replace('\\n', ' ').replace(',', ' ').replace('.com', '.com ')\n",
    "    page = page.translate(str.maketrans('', '', digits))\n",
    "    \n",
    "    \n",
    "    \n",
    "    pattern =r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,6}\\b'\n",
    "    emails = re.findall(pattern, page)\n",
    "    \n",
    "    brute_force = []\n",
    "    for name in uni_names:\n",
    "        if name in page:\n",
    "            brute_force.append(name)\n",
    "    \n",
    "    return list(set(emails) | set(brute_force))\n",
    "    \n",
    "\n",
    "    # page = page.replace('\\n', ' ').replace(',', ' ').replace('.com', '.com ')\n",
    "    # page = page.translate(str.maketrans('', '', digits))\n",
    "    # pattern =r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,6}\\b'\n",
    "    # out = re.findall(pattern, page)\n",
    "\n",
    "    # page_lst = page.split(' ')\n",
    "    # for i in range(len(page_lst)):\n",
    "    #     if page_lst[i].lower() in ['hospital','university','institute','school','academy', 'department', 'uc', 'tech', 'emory', ]:\n",
    "    #         out.append(re.sub(\"[^a-zA-Z]+\", \"\", ' '.join(page_lst[i-6:i+6]).lower()))\n",
    "    # return list(set(out))\n",
    "def get_authors(lst):\n",
    "    out = []\n",
    "    for auth in lst:\n",
    "        out.append(auth['given']+ ' '+auth['family'])\n",
    "    return out\n",
    "def get_subject(lst):\n",
    "    out = []\n",
    "    for sub in lst:\n",
    "        out.append(sub['text'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "old\n",
      "0\n",
      "old\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "old\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "old\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "old\n",
      "39\n",
      "40\n",
      "41\n",
      "old\n",
      "42\n",
      "old\n",
      "43\n",
      "old\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "old\n",
      "48\n",
      "old\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "old\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "old\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "old\n",
      "66\n",
      "67\n",
      "68\n",
      "old\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "old\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "old\n",
      "89\n",
      "old\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "here\n",
      "100\n",
      "old\n",
      "101\n",
      "old\n",
      "102\n",
      "old\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "old\n",
      "107\n",
      "108\n",
      "109\n",
      "old\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "old\n",
      "121\n",
      "old\n",
      "122\n",
      "old\n",
      "123\n",
      "old\n",
      "124\n",
      "old\n",
      "125\n",
      "old\n",
      "126\n",
      "old\n",
      "127\n",
      "128\n",
      "old\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "old\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "old\n",
      "143\n",
      "144\n",
      "old\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "old\n",
      "150\n",
      "old\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "old\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "old\n",
      "161\n",
      "162\n",
      "old\n",
      "163\n",
      "164\n",
      "old\n",
      "165\n",
      "old\n",
      "166\n",
      "167\n",
      "old\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "old\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "old\n",
      "184\n",
      "old\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "old\n",
      "193\n",
      "194\n",
      "old\n",
      "195\n",
      "old\n",
      "196\n",
      "197\n",
      "198\n",
      "old\n",
      "199\n",
      "here\n",
      "old\n",
      "200\n",
      "old\n",
      "201\n",
      "202\n",
      "203\n",
      "old\n",
      "204\n",
      "old\n",
      "205\n",
      "old\n",
      "206\n",
      "old\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "old\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "old\n",
      "242\n",
      "243\n",
      "old\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "old\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "old\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "old\n",
      "263\n",
      "264\n",
      "265\n",
      "old\n",
      "266\n",
      "old\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "old\n",
      "277\n",
      "278\n",
      "old\n",
      "279\n",
      "280\n",
      "old\n",
      "281\n",
      "282\n",
      "283\n",
      "old\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "old\n",
      "295\n",
      "296\n",
      "old\n",
      "297\n",
      "298\n",
      "299\n",
      "here\n",
      "old\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "old\n",
      "306\n",
      "old\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "old\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "old\n",
      "316\n",
      "317\n",
      "old\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "old\n",
      "322\n",
      "323\n",
      "old\n",
      "324\n",
      "old\n",
      "325\n",
      "326\n",
      "old\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "old\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "old\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "old\n",
      "348\n",
      "old\n",
      "349\n",
      "old\n",
      "350\n",
      "351\n",
      "old\n",
      "352\n",
      "old\n",
      "353\n",
      "354\n",
      "355\n",
      "old\n",
      "356\n",
      "357\n",
      "358\n",
      "old\n",
      "359\n",
      "360\n",
      "361\n",
      "old\n",
      "362\n",
      "363\n",
      "old\n",
      "364\n",
      "365\n",
      "old\n",
      "366\n",
      "old\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "old\n",
      "371\n",
      "old\n",
      "372\n",
      "373\n",
      "374\n",
      "old\n",
      "375\n",
      "376\n",
      "377\n",
      "old\n",
      "378\n",
      "379\n",
      "old\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "old\n",
      "387\n",
      "old\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "old\n",
      "392\n",
      "old\n",
      "393\n",
      "old\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "old\n",
      "399\n",
      "here\n",
      "old\n",
      "400\n",
      "old\n",
      "401\n",
      "402\n",
      "old\n",
      "403\n",
      "404\n",
      "old\n",
      "405\n",
      "406\n",
      "407\n",
      "old\n",
      "408\n",
      "409\n",
      "old\n",
      "410\n",
      "old\n",
      "411\n",
      "old\n",
      "412\n",
      "old\n",
      "413\n",
      "414\n",
      "old\n",
      "415\n",
      "old\n",
      "416\n",
      "old\n",
      "417\n",
      "old\n",
      "418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0xc6ff73 for key /Im187\n",
      "Multiple definitions in dictionary at byte 0xc6ff82 for key /Im187\n",
      "Multiple definitions in dictionary at byte 0xc6ff91 for key /Im187\n",
      "Multiple definitions in dictionary at byte 0xd081fe for key /Im198\n",
      "Multiple definitions in dictionary at byte 0xd0820d for key /Im198\n",
      "Multiple definitions in dictionary at byte 0x13753cd for key /Im265\n",
      "Multiple definitions in dictionary at byte 0x13753dc for key /Im265\n",
      "Multiple definitions in dictionary at byte 0x13753eb for key /Im265\n",
      "Multiple definitions in dictionary at byte 0x13e397a for key /Im292\n",
      "Multiple definitions in dictionary at byte 0x13e3989 for key /Im292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old\n",
      "419\n",
      "420\n",
      "old\n",
      "421\n",
      "old\n",
      "422\n",
      "old\n",
      "423\n",
      "424\n",
      "425\n",
      "old\n",
      "426\n",
      "old\n",
      "427\n",
      "428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRef object at 1019668 can not be read, some object may be missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old\n",
      "429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRef object at 944179 can not be read, some object may be missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old\n",
      "430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XRef object at 1160514 can not be read, some object may be missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "old\n",
      "441\n",
      "442\n",
      "old\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "old\n",
      "453\n",
      "454\n",
      "455\n",
      "old\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "old\n",
      "461\n",
      "462\n",
      "463\n",
      "old\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "old\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "old\n",
      "474\n",
      "old\n",
      "475\n",
      "476\n",
      "old\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "old\n",
      "482\n",
      "483\n",
      "old\n",
      "484\n",
      "485\n",
      "old\n",
      "486\n",
      "487\n",
      "old\n",
      "488\n",
      "old\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "here\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "old\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "old\n",
      "529\n",
      "530\n",
      "old\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "old\n",
      "535\n",
      "536\n",
      "old\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "old\n",
      "541\n",
      "542\n",
      "543\n",
      "old\n",
      "544\n",
      "545\n",
      "546\n",
      "old\n",
      "547\n",
      "old\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "old\n",
      "559\n",
      "560\n",
      "old\n",
      "561\n",
      "old\n",
      "562\n",
      "old\n",
      "563\n",
      "old\n",
      "564\n",
      "old\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "old\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "old\n",
      "584\n",
      "585\n",
      "old\n",
      "586\n",
      "old\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "here\n",
      "old\n",
      "600\n",
      "old\n",
      "601\n",
      "old\n",
      "602\n",
      "old\n",
      "603\n",
      "old\n",
      "604\n",
      "old\n",
      "605\n",
      "606\n",
      "607\n",
      "old\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "old\n",
      "612\n",
      "old\n",
      "613\n",
      "old\n",
      "614\n",
      "old\n",
      "615\n",
      "old\n",
      "616\n",
      "617\n",
      "old\n",
      "618\n",
      "old\n",
      "619\n",
      "old\n",
      "620\n",
      "621\n",
      "old\n",
      "622\n",
      "old\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "old\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "old\n",
      "632\n",
      "old\n",
      "633\n",
      "634\n",
      "old\n",
      "635\n",
      "old\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "old\n",
      "640\n",
      "old\n",
      "641\n",
      "old\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "old\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "old\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "old\n",
      "656\n",
      "657\n",
      "658\n",
      "old\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "old\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "old\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "old\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "old\n",
      "697\n",
      "698\n",
      "old\n",
      "699\n",
      "here\n",
      "old\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "old\n",
      "704\n",
      "old\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "old\n",
      "709\n",
      "710\n",
      "711\n",
      "old\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "old\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "old\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "old\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "old\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "old\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "old\n",
      "775\n",
      "776\n",
      "old\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "old\n",
      "783\n",
      "old\n",
      "784\n",
      "old\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "here\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "old\n",
      "808\n",
      "old\n",
      "809\n",
      "810\n",
      "811\n",
      "old\n",
      "812\n",
      "813\n",
      "old\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "old\n",
      "824\n",
      "825\n",
      "old\n",
      "826\n",
      "827\n",
      "old\n",
      "828\n",
      "829\n",
      "830\n",
      "old\n",
      "831\n",
      "832\n",
      "833\n",
      "old\n",
      "834\n",
      "old\n",
      "835\n",
      "old\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "old\n",
      "841\n",
      "842\n",
      "843\n",
      "old\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "old\n",
      "849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting cache for 0 194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "old\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "old\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "old\n",
      "881\n",
      "old\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "old\n",
      "898\n",
      "old\n",
      "899\n",
      "here\n",
      "old\n",
      "900\n",
      "old\n",
      "901\n",
      "old\n",
      "902\n",
      "903\n",
      "old\n",
      "904\n",
      "old\n",
      "905\n",
      "906\n",
      "907\n",
      "old\n",
      "908\n",
      "old\n",
      "909\n",
      "old\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "old\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "old\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "old\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\Documents\\ResearchFinder\\research_finder.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#W1sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#W1sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m df \u001b[39m=\u001b[39m get_osf(\u001b[39m14\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#W1sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_parquet(\u001b[39m'\u001b[39;49m\u001b[39mosf_data.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:2976\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2890\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2891\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2972\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2973\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2974\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2976\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2977\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2978\u001b[0m     path,\n\u001b[0;32m   2979\u001b[0m     engine,\n\u001b[0;32m   2980\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   2981\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   2982\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m   2983\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2984\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2985\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:426\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(partition_cols, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    425\u001b[0m     partition_cols \u001b[39m=\u001b[39m [partition_cols]\n\u001b[1;32m--> 426\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[0;32m    428\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m    430\u001b[0m impl\u001b[39m.\u001b[39mwrite(\n\u001b[0;32m    431\u001b[0m     df,\n\u001b[0;32m    432\u001b[0m     path_or_buf,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    438\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:52\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     50\u001b[0m             error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(err)\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to find a usable engine; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtried using: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA suitable version of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msupport.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to import the above resulted in these errors:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00merror_msgs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     )\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "pub = \"\"\n",
    "def get_osf(num_days):\n",
    "    OSF_token = 'XhAMbzE7SI0KcHYGLgn1oI5EeMqWbtiJuUOTjR1VdLDnF343kgT9OMhTwHiTY5lfd1ma1c'\n",
    "    api_url = \"https://api.osf.io/v2/preprints/\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OSF_token}\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "            \"page[size]\": 100,# this is max ==100\n",
    "            \"filter[date_published][gte]\":(datetime.now() - timedelta(days=num_days)).isoformat()\n",
    "            \n",
    "        }\n",
    "    out = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        response = requests.get(api_url, headers=headers, params=params)\n",
    "        data = response.json()\n",
    "        print('here')\n",
    "        for publication in data[\"data\"]:\n",
    "            paper_id = publication['id']\n",
    "            # global pub\n",
    "            # pub = publication\n",
    "            #try:\n",
    "            data_auths = requests.get(f'https://api.osf.io/v2/preprints/{paper_id}/citation/', headers=headers).json()['data']['attributes']['author']\n",
    "            subjects = publication['attributes']['subjects'][0]\n",
    "            title = publication[\"attributes\"][\"title\"]\n",
    "            abstract = publication['attributes']['description']\n",
    "            url = publication['links']['html']\n",
    "            date = publication['attributes']['date_published']\n",
    "            emails = osf_download(publication['relationships']['primary_file']['links']['related']['href'][28:])\n",
    "            #if i%10 ==0:\n",
    "            print(i)\n",
    "            i+=1\n",
    "            out.append([paper_id, data_auths, subjects, title, abstract, url, date, emails])\n",
    "            # except:\n",
    "            #     pass\n",
    "        try:\n",
    "            api_url =data['links']['next']\n",
    "            if api_url == None:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "        #break#remove before full use, just for testing\n",
    "    df = pd.DataFrame(out, columns=['paper_id', 'authors', 'subjects', 'title', 'abstract', 'url', 'date', 'affiliations'])\n",
    "    df['authors'] = df['authors'].apply(get_authors)\n",
    "    df['subjects'] = df['subjects'].apply(get_subject)\n",
    "    df['source'] = 'OSF'\n",
    "    return df\n",
    "df = get_osf(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('osf_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['affiliations'].astype(str).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2:52 on old system\n",
    "# then 3:21\n",
    "#brute force old 2:21\n",
    "# brute force new 2:19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3:09 old only\n",
    "#new 4:02 try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do\n",
    "# llama for abstract probably just for top 20 per user\n",
    "# fuzzy search between stated wants and subjects\n",
    "# then just need to make Plotly dash site\n",
    "# also need to get Patent, trademarks, grants data as cake\n",
    "# primary focus on grant db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now w/ ARXIV since OSF seems to miss it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "ex\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_arxiv_catchup_per_subject(soup):\n",
    "    print('here')\n",
    "    out = []\n",
    "    data = soup.text.split('\\n arXiv:')\n",
    "    for entry in data[1:]:\n",
    "        entry = entry.split('\\n\\n')\n",
    "        try:\n",
    "            entry.remove(\"\")\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            entry.remove(\" \")\n",
    "        except:\n",
    "            pass\n",
    "        entry = [x for x in entry if not 'Comments: ' in x]\n",
    "\n",
    "        try:\n",
    "            entry_data = {\n",
    "                'paper_id':entry[0].split(\" \")[0],\n",
    "                'title':entry[1].replace(\"Title: \", '').replace(\"\\n\", ''),\n",
    "                'authors':entry[2].replace(\"Authors:\", '').replace(\"\\n\", '').split(', '),\n",
    "                'subjects':entry[3].replace(\"Subjects: \", '').replace(\"\\n\", ''),\n",
    "                'abstract':entry[4].replace(\"\\n\", ''),\n",
    "                'affiliations': download_arxiv(entry[0].split(\" \")[0])\n",
    "            }\n",
    "            out.append(entry_data)\n",
    "            print('ex')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return out\n",
    "def get_arxiv_per_day(date):\n",
    "    out = []\n",
    "    for subject in ['q-fin', 'cs', 'math', 'q-bio', 'hep-th', 'stat', 'econ', 'eess']: \n",
    "        url = f\"https://arxiv.org/catchup?smonth={str(date.month)}&group=grp_&sday={str(date.day)}&archive={subject}&method=with&syear={str(date.year)}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        out.extend(get_arxiv_catchup_per_subject(soup))\n",
    "    df = pd.DataFrame.from_records(out)\n",
    "    df['date']= date.strftime('%Y-%m-%d')\n",
    "    df['source'] = 'arXiv'\n",
    "    return df\n",
    "date = datetime.utcnow()-timedelta(days=1)\n",
    "df1 = get_arxiv_per_day(date)\n",
    "#df1.to_parquet('arxiv_data.parquet', index=False)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "1115\n",
      "1120\n",
      "here\n",
      "1125\n",
      "1130\n",
      "1135\n",
      "1140\n",
      "1145\n",
      "1150\n",
      "1155\n",
      "1160\n",
      "1165\n",
      "1170\n",
      "1175\n",
      "1180\n",
      "1185\n",
      "1190\n",
      "1195\n",
      "1200\n",
      "1205\n",
      "1210\n",
      "1215\n",
      "1220\n",
      "1225\n",
      "1230\n",
      "1235\n",
      "1240\n",
      "1245\n",
      "1250\n",
      "1255\n",
      "1260\n",
      "1265\n",
      "1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x2cdef for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x2d077 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x2d141 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x2d20b for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275\n",
      "1280\n",
      "1285\n",
      "1290\n",
      "1295\n",
      "1300\n",
      "1305\n",
      "1310\n",
      "1315\n",
      "1320\n",
      "1325\n",
      "1330\n",
      "1335\n",
      "1340\n",
      "1345\n",
      "1350\n",
      "1355\n",
      "1360\n",
      "1365\n",
      "1370\n",
      "1375\n",
      "1380\n",
      "1385\n",
      "1390\n",
      "1395\n",
      "1400\n",
      "1405\n",
      "1410\n",
      "1415\n",
      "1420\n",
      "1425\n",
      "1430\n",
      "1435\n",
      "1440\n",
      "1445\n",
      "1450\n",
      "1455\n",
      "1460\n",
      "1465\n",
      "1470\n",
      "1475\n",
      "1480\n",
      "1485\n",
      "1490\n",
      "1495\n",
      "1500\n",
      "1505\n",
      "1510\n",
      "1515\n",
      "1520\n",
      "1525\n",
      "1530\n",
      "1535\n",
      "1540\n",
      "1545\n",
      "1550\n",
      "1555\n",
      "1560\n",
      "1565\n",
      "1570\n",
      "1575\n",
      "1580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585\n",
      "1590\n",
      "1595\n",
      "1600\n",
      "1605\n",
      "1610\n",
      "1615\n",
      "1620\n",
      "1625\n",
      "1630\n",
      "1635\n",
      "1640\n",
      "1645\n",
      "1650\n",
      "1655\n",
      "1660\n",
      "1665\n",
      "1670\n",
      "1675\n",
      "1680\n",
      "1685\n",
      "1690\n",
      "1695\n",
      "1700\n",
      "1705\n",
      "1710\n",
      "1715\n",
      "1720\n",
      "1725\n",
      "1730\n",
      "1735\n",
      "1740\n",
      "1745\n",
      "here\n",
      "1750\n",
      "1755\n",
      "1760\n",
      "1765\n",
      "1770\n",
      "1775\n",
      "1780\n",
      "1785\n",
      "1790\n",
      "1795\n",
      "1800\n",
      "1805\n",
      "1810\n",
      "1815\n",
      "1820\n",
      "1825\n",
      "1830\n",
      "1835\n",
      "1840\n",
      "1845\n",
      "1850\n",
      "1855\n",
      "1860\n",
      "1865\n",
      "1870\n",
      "1875\n",
      "1880\n",
      "1885\n",
      "1890\n",
      "1895\n",
      "1900\n",
      "1905\n",
      "1910\n",
      "1915\n",
      "1920\n",
      "1925\n",
      "1930\n",
      "1935\n",
      "1940\n",
      "1945\n",
      "1950\n",
      "1955\n",
      "1960\n",
      "1965\n",
      "1970\n",
      "1975\n",
      "1980\n",
      "1985\n",
      "1990\n",
      "1995\n",
      "2000\n",
      "2005\n",
      "2010\n",
      "2015\n",
      "2020\n",
      "2025\n",
      "2030\n",
      "2035\n",
      "2040\n",
      "2045\n",
      "2050\n",
      "2055\n",
      "2060\n",
      "2065\n",
      "here\n",
      "2070\n",
      "2075\n",
      "2080\n",
      "2085\n",
      "2090\n",
      "here\n",
      "2095\n",
      "2100\n",
      "2105\n",
      "2110\n",
      "2115\n",
      "2120\n",
      "2125\n",
      "2130\n",
      "2135\n",
      "2140\n",
      "2145\n",
      "2150\n",
      "2155\n",
      "2160\n",
      "2165\n",
      "2170\n",
      "here\n",
      "2175\n",
      "2180\n",
      "2185\n",
      "2190\n",
      "2195\n",
      "2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x53fc1 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x54073 for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2205\n",
      "2210\n",
      "2215\n",
      "2220\n",
      "2225\n",
      "here\n",
      "2230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x53fc1 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x54073 for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235\n",
      "here\n",
      "2240\n",
      "2245\n",
      "2250\n",
      "2255\n",
      "2260\n",
      "2265\n",
      "2270\n",
      "2275\n",
      "2280\n",
      "2285\n",
      "2290\n",
      "2295\n",
      "2300\n",
      "2305\n",
      "2310\n",
      "2315\n",
      "2320\n",
      "2325\n",
      "2330\n",
      "2335\n",
      "2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345\n",
      "2350\n",
      "2355\n",
      "2360\n",
      "2365\n",
      "2370\n",
      "2375\n",
      "here\n",
      "2380\n",
      "2385\n",
      "2390\n",
      "2395\n",
      "here\n",
      "2400\n",
      "2405\n",
      "2410\n",
      "2415\n",
      "2420\n",
      "2425\n",
      "2430\n",
      "2435\n",
      "2440\n",
      "2445\n",
      "2450\n",
      "2455\n",
      "2460\n",
      "2465\n",
      "2470\n",
      "2475\n",
      "2480\n",
      "2485\n",
      "2490\n",
      "2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2505\n",
      "2510\n",
      "2515\n",
      "2520\n",
      "2525\n",
      "2530\n",
      "2535\n",
      "2540\n",
      "2545\n",
      "2550\n",
      "2555\n",
      "2560\n",
      "2565\n",
      "2570\n",
      "2575\n",
      "2580\n",
      "2585\n",
      "2590\n",
      "2595\n",
      "2600\n",
      "2605\n",
      "2610\n",
      "2615\n",
      "2620\n",
      "2625\n",
      "2630\n",
      "2635\n",
      "2640\n",
      "2645\n",
      "2650\n",
      "2655\n",
      "2660\n",
      "2665\n",
      "2670\n",
      "2675\n",
      "2680\n",
      "2685\n",
      "2690\n",
      "2695\n",
      "2700\n",
      "2705\n",
      "2710\n",
      "2715\n",
      "2720\n",
      "2725\n",
      "2730\n",
      "2735\n",
      "2740\n",
      "2745\n",
      "2750\n",
      "2755\n",
      "2760\n",
      "2765\n",
      "2770\n",
      "2775\n",
      "2780\n",
      "2785\n",
      "2790\n",
      "2795\n",
      "2800\n",
      "2805\n",
      "2810\n",
      "2815\n",
      "2820\n",
      "2825\n",
      "2830\n",
      "2835\n",
      "2840\n",
      "2845\n",
      "2850\n",
      "2855\n",
      "2860\n",
      "2865\n",
      "2870\n",
      "2875\n",
      "2880\n",
      "2885\n",
      "2890\n",
      "2895\n",
      "2900\n",
      "2905\n",
      "2910\n",
      "2915\n",
      "here\n",
      "2920\n",
      "2925\n",
      "2930\n",
      "2935\n",
      "2940\n",
      "2945\n",
      "2950\n",
      "2955\n",
      "2960\n",
      "2965\n",
      "2970\n",
      "2975\n",
      "2980\n",
      "2985\n",
      "2990\n",
      "2995\n",
      "3000\n",
      "3005\n",
      "3010\n",
      "3015\n",
      "3020\n",
      "3025\n",
      "3030\n",
      "3035\n",
      "3040\n",
      "3045\n",
      "3050\n",
      "3055\n",
      "3060\n",
      "3065\n",
      "3070\n",
      "3075\n",
      "3080\n",
      "3085\n",
      "3090\n",
      "3095\n",
      "3100\n",
      "3105\n",
      "3110\n",
      "3115\n",
      "3120\n",
      "3125\n",
      "3130\n",
      "3135\n",
      "3140\n",
      "3145\n",
      "3150\n",
      "3155\n",
      "3160\n",
      "3165\n",
      "3170\n",
      "3175\n",
      "3180\n",
      "3185\n",
      "3190\n",
      "3195\n",
      "3200\n",
      "here\n",
      "3205\n",
      "3210\n",
      "3215\n",
      "3220\n",
      "3225\n",
      "3230\n",
      "3235\n",
      "3240\n",
      "3245\n",
      "here\n",
      "3250\n",
      "3255\n",
      "3260\n",
      "3265\n",
      "3270\n",
      "3275\n",
      "3280\n",
      "3285\n",
      "3290\n",
      "3295\n",
      "3300\n",
      "here\n",
      "3305\n",
      "3310\n",
      "3315\n",
      "3320\n",
      "3325\n",
      "3330\n",
      "3335\n",
      "3340\n",
      "3345\n",
      "3350\n",
      "3355\n",
      "3360\n",
      "3365\n",
      "here\n",
      "3370\n",
      "3375\n",
      "3380\n",
      "3385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x53fc1 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x54073 for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3390\n",
      "here\n",
      "3395\n",
      "3400\n",
      "3405\n",
      "3410\n",
      "3415\n",
      "3420\n",
      "3425\n",
      "3430\n",
      "3435\n",
      "3440\n",
      "3445\n",
      "3450\n",
      "3455\n",
      "3460\n",
      "3465\n",
      "3470\n",
      "3475\n",
      "3480\n",
      "3485\n",
      "3490\n",
      "here\n",
      "3495\n",
      "3500\n",
      "3505\n",
      "3510\n",
      "3515\n",
      "3520\n",
      "3525\n",
      "here\n",
      "3530\n",
      "3535\n",
      "3540\n",
      "3545\n",
      "3550\n",
      "3555\n",
      "3560\n",
      "3565\n",
      "3570\n",
      "3575\n",
      "3580\n",
      "3585\n",
      "3590\n",
      "3595\n",
      "3600\n",
      "3605\n",
      "3610\n",
      "3615\n",
      "3620\n",
      "3625\n",
      "3630\n",
      "3635\n",
      "3640\n",
      "3645\n",
      "3650\n",
      "3655\n",
      "3660\n",
      "3665\n",
      "3670\n",
      "3675\n",
      "3680\n",
      "3685\n",
      "3690\n",
      "3695\n",
      "3700\n",
      "3705\n",
      "3710\n",
      "3715\n",
      "3720\n",
      "3725\n",
      "3730\n",
      "3735\n",
      "3740\n",
      "3745\n",
      "3750\n",
      "3755\n",
      "3760\n",
      "3765\n",
      "3770\n",
      "3775\n",
      "3780\n",
      "3785\n",
      "3790\n",
      "3795\n",
      "3800\n",
      "3805\n",
      "3810\n",
      "3815\n",
      "3820\n",
      "3825\n",
      "3830\n",
      "3835\n",
      "3840\n",
      "3845\n",
      "3850\n",
      "3855\n",
      "3860\n",
      "3865\n",
      "3870\n",
      "3875\n",
      "3880\n",
      "3885\n",
      "3890\n",
      "3895\n",
      "3900\n",
      "3905\n",
      "3910\n",
      "3915\n",
      "3920\n",
      "3925\n",
      "3930\n",
      "3935\n",
      "3940\n",
      "3945\n",
      "3950\n",
      "3955\n",
      "3960\n",
      "3965\n",
      "3970\n",
      "3975\n",
      "3980\n",
      "3985\n",
      "3990\n",
      "3995\n",
      "4000\n",
      "4005\n",
      "4010\n",
      "4015\n",
      "4020\n",
      "4025\n",
      "4030\n",
      "4035\n",
      "4040\n",
      "4045\n",
      "4050\n",
      "4055\n",
      "4060\n",
      "4065\n",
      "4070\n",
      "4075\n",
      "4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-40') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4085\n",
      "4090\n",
      "4095\n",
      "4100\n",
      "4105\n",
      "4110\n",
      "4115\n",
      "4120\n",
      "4125\n",
      "4130\n",
      "4135\n",
      "4140\n",
      "4145\n",
      "4150\n",
      "4155\n",
      "4160\n",
      "4165\n",
      "here\n",
      "4170\n",
      "4175\n",
      "4180\n",
      "4185\n",
      "4190\n",
      "4195\n",
      "4200\n",
      "4205\n",
      "4210\n",
      "4215\n",
      "4220\n",
      "4225\n",
      "4230\n",
      "4235\n",
      "4240\n",
      "4245\n",
      "4250\n",
      "4255\n",
      "4260\n",
      "4265\n",
      "4270\n",
      "4275\n",
      "4280\n",
      "4285\n",
      "4290\n",
      "4295\n",
      "4300\n",
      "4305\n",
      "4310\n",
      "4315\n",
      "4320\n",
      "4325\n",
      "4330\n",
      "4335\n",
      "4340\n",
      "4345\n",
      "4350\n",
      "4355\n",
      "4360\n",
      "4365\n",
      "4370\n",
      "4375\n",
      "4380\n",
      "4385\n",
      "4390\n",
      "4395\n",
      "4400\n",
      "4405\n",
      "4410\n",
      "4415\n",
      "4420\n",
      "4425\n",
      "4430\n",
      "4435\n",
      "4440\n",
      "4445\n",
      "4450\n",
      "4455\n",
      "4460\n",
      "4465\n",
      "4470\n",
      "4475\n",
      "4480\n",
      "4485\n",
      "4490\n",
      "4495\n",
      "4500\n",
      "here\n",
      "4505\n",
      "4510\n",
      "4515\n",
      "4520\n",
      "4525\n",
      "4530\n",
      "4535\n",
      "4540\n",
      "4545\n",
      "4550\n",
      "4555\n",
      "here\n",
      "4560\n",
      "4565\n",
      "4570\n",
      "4575\n",
      "4580\n",
      "4585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x5a8e9 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x5bec9 for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4590\n",
      "4595\n",
      "4600\n",
      "4605\n",
      "4610\n",
      "4615\n",
      "here\n",
      "4620\n",
      "4625\n",
      "4630\n",
      "4635\n",
      "4640\n",
      "4645\n",
      "4650\n",
      "4655\n",
      "4660\n",
      "4665\n",
      "4670\n",
      "4675\n",
      "4680\n",
      "4685\n",
      "here\n",
      "4690\n",
      "4695\n",
      "4700\n",
      "4705\n",
      "4710\n",
      "4715\n",
      "4720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x53fc1 for key /Rotate\n",
      "Multiple definitions in dictionary at byte 0x54073 for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4725\n",
      "here\n",
      "4730\n",
      "4735\n",
      "4740\n",
      "4745\n",
      "4750\n",
      "4755\n",
      "4760\n",
      "4765\n",
      "4770\n",
      "4775\n",
      "4780\n",
      "4785\n",
      "4790\n",
      "4795\n",
      "4800\n",
      "4805\n",
      "4810\n",
      "4815\n",
      "4820\n",
      "4825\n",
      "4830\n",
      "4835\n",
      "4840\n",
      "4845\n",
      "4850\n",
      "here\n",
      "4855\n",
      "4860\n",
      "4865\n",
      "4870\n",
      "4875\n",
      "4880\n",
      "4885\n",
      "4890\n",
      "4895\n",
      "4900\n",
      "4905\n",
      "4910\n",
      "here\n",
      "4915\n",
      "4920\n",
      "4925\n",
      "4930\n",
      "4935\n",
      "4940\n",
      "4945\n",
      "4950\n",
      "4955\n",
      "4960\n",
      "4965\n",
      "4970\n",
      "4975\n",
      "4980\n",
      "4985\n",
      "4990\n",
      "4995\n",
      "5000\n",
      "5005\n",
      "5010\n",
      "5015\n",
      "5020\n",
      "5025\n",
      "5030\n",
      "5035\n",
      "5040\n",
      "5045\n",
      "5050\n",
      "5055\n",
      "5060\n",
      "5065\n",
      "5070\n",
      "5075\n",
      "5080\n",
      "5085\n",
      "5090\n",
      "5095\n",
      "5100\n",
      "5105\n",
      "5110\n",
      "5115\n",
      "5120\n",
      "5125\n",
      "5130\n",
      "5135\n",
      "5140\n",
      "5145\n",
      "5150\n",
      "5155\n",
      "5160\n",
      "5165\n",
      "5170\n",
      "5175\n",
      "5180\n",
      "5185\n",
      "5190\n",
      "5195\n",
      "5200\n",
      "5205\n",
      "5210\n",
      "5215\n",
      "5220\n",
      "5225\n",
      "5230\n",
      "5235\n",
      "5240\n",
      "5245\n",
      "5250\n",
      "5255\n",
      "5260\n",
      "5265\n",
      "5270\n",
      "5275\n",
      "5280\n",
      "5285\n",
      "5290\n",
      "5295\n",
      "5300\n",
      "5305\n",
      "5310\n",
      "5315\n",
      "5320\n",
      "5325\n",
      "5330\n",
      "5335\n",
      "5340\n",
      "5345\n",
      "5350\n",
      "5355\n",
      "5360\n",
      "5365\n",
      "5370\n",
      "5375\n",
      "5380\n",
      "5385\n",
      "5390\n",
      "5395\n",
      "5400\n",
      "5405\n",
      "5410\n",
      "5415\n",
      "5420\n",
      "5425\n",
      "5430\n",
      "5435\n",
      "5440\n",
      "5445\n",
      "5450\n",
      "5455\n",
      "5460\n",
      "5465\n",
      "5470\n",
      "5475\n",
      "5480\n",
      "5485\n",
      "5490\n",
      "5495\n",
      "5500\n",
      "5505\n",
      "5510\n",
      "5515\n",
      "5520\n",
      "5525\n",
      "5530\n",
      "5535\n",
      "5540\n",
      "5545\n",
      "here\n",
      "5550\n",
      "5555\n",
      "5560\n",
      "5565\n",
      "5570\n",
      "5575\n",
      "5580\n",
      "5585\n",
      "5590\n",
      "5595\n",
      "5600\n",
      "5605\n",
      "5610\n",
      "5615\n",
      "5620\n",
      "5625\n",
      "5630\n",
      "5635\n",
      "5640\n",
      "5645\n",
      "5650\n",
      "5655\n",
      "5660\n",
      "5665\n",
      "5670\n",
      "5675\n",
      "5680\n",
      "5685\n",
      "5690\n",
      "5695\n",
      "5700\n",
      "5705\n",
      "5710\n",
      "5715\n",
      "5720\n",
      "5725\n",
      "5730\n",
      "5735\n",
      "5740\n",
      "5745\n",
      "5750\n",
      "5755\n",
      "5760\n",
      "5765\n",
      "5770\n",
      "5775\n",
      "5780\n",
      "5785\n",
      "5790\n",
      "5795\n",
      "5800\n",
      "5805\n",
      "5810\n",
      "5815\n",
      "5820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0xd22db for key /Rotate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5825\n",
      "5830\n",
      "5835\n",
      "5840\n",
      "5845\n",
      "5850\n",
      "5855\n",
      "5860\n",
      "5865\n",
      "5870\n",
      "5875\n",
      "5880\n",
      "5885\n",
      "5890\n",
      "5895\n",
      "5900\n",
      "5905\n",
      "5910\n",
      "5915\n",
      "5920\n",
      "5925\n",
      "5930\n",
      "5935\n",
      "5940\n",
      "5945\n",
      "5950\n",
      "5955\n",
      "5960\n",
      "5965\n",
      "5970\n",
      "5975\n",
      "5980\n",
      "5985\n",
      "5990\n",
      "5995\n",
      "6000\n",
      "6005\n",
      "6010\n",
      "6015\n",
      "6020\n",
      "6025\n",
      "6030\n",
      "6035\n",
      "6040\n",
      "6045\n",
      "6050\n",
      "6055\n",
      "6060\n",
      "6065\n",
      "6070\n",
      "6075\n",
      "6080\n",
      "6085\n",
      "6090\n",
      "6095\n",
      "6100\n",
      "6105\n",
      "6110\n",
      "6115\n",
      "6120\n",
      "6125\n",
      "6130\n",
      "6135\n",
      "6140\n",
      "6145\n",
      "6150\n",
      "6155\n",
      "6160\n",
      "6165\n",
      "6170\n",
      "6175\n",
      "here\n",
      "6180\n",
      "6185\n",
      "6190\n",
      "6195\n",
      "6200\n",
      "6205\n",
      "6210\n",
      "6215\n",
      "6220\n",
      "6225\n",
      "6230\n",
      "6235\n",
      "here\n",
      "6240\n",
      "6245\n",
      "6250\n",
      "6255\n",
      "6260\n",
      "6265\n",
      "6270\n",
      "6275\n",
      "6280\n",
      "6285\n",
      "6290\n",
      "6295\n",
      "here\n",
      "6300\n",
      "6305\n",
      "6310\n",
      "6315\n",
      "6320\n",
      "6325\n",
      "6330\n",
      "6335\n",
      "6340\n",
      "6345\n",
      "6350\n",
      "6355\n",
      "6360\n",
      "6365\n",
      "6370\n",
      "6375\n",
      "6380\n",
      "6385\n",
      "6390\n",
      "6395\n",
      "6400\n",
      "6405\n",
      "6410\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "def get_arxiv_days_back(num_back):\n",
    "    df = pd.DataFrame()\n",
    "    date = datetime.utcnow()\n",
    "    for i in range(num_back):\n",
    "        df = pd.concat([df, get_arxiv_per_day(date-timedelta(days=i))])\n",
    "        df.to_parquet('arxiv_data_long_temp.parquet', index=False)\n",
    "    return df\n",
    "df3 =get_arxiv_days_back(7)\n",
    "df3.to_parquet('arxiv_data_long.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df,df3])\n",
    "df2.to_parquet('combined_data.parquet', index=False)\n",
    "df2 = df2.drop_duplicates(subset='paper_id')\n",
    "df2.to_parquet('de_duped.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>subjects</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k5ghb</td>\n",
       "      <td>[Setblon Tembang]</td>\n",
       "      <td>[Social and Behavioral Sciences, Psychology, O...</td>\n",
       "      <td>Pentingnya Konseling Eksistensial-Humanistik d...</td>\n",
       "      <td>Penelitian ini merupakan penelitian kulitatif ...</td>\n",
       "      <td>https://osf.io/k5ghb/</td>\n",
       "      <td>2023-09-22T15:11:44.880299</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yjga5</td>\n",
       "      <td>[Emma Thornton, Kimberly Petersen, Jose Marque...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>Do patterns of adolescent participation in art...</td>\n",
       "      <td>Using latent class analysis, we sought to esta...</td>\n",
       "      <td>https://psyarxiv.com/yjga5/</td>\n",
       "      <td>2023-09-22T15:12:25.767233</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ptdg</td>\n",
       "      <td>[Jean-François Bonnefon, Iyad Rahwan, Azim Sha...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>The moral psychology of Artificial Intelligence</td>\n",
       "      <td>Moral psychology was shaped around three categ...</td>\n",
       "      <td>https://psyarxiv.com/8ptdg/</td>\n",
       "      <td>2023-09-22T15:10:52.677112</td>\n",
       "      <td>[jean-francois.bonnefon@tse-fr.eu, VT]</td>\n",
       "      <td>OSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hk7fp</td>\n",
       "      <td>[Jordan D Klein, Anjarasoa Rasoanomenjanahary]</td>\n",
       "      <td>[Medicine and Health Sciences, Public Health, ...</td>\n",
       "      <td>Climate Change and Health Transitions: Evidenc...</td>\n",
       "      <td>BACKGROUND\\nGlobal climate change poses grave ...</td>\n",
       "      <td>https://osf.io/hk7fp/</td>\n",
       "      <td>2023-09-22T14:32:27.106595</td>\n",
       "      <td>[jdklein@princeton.edu, OU, UND , IC, anjaraso...</td>\n",
       "      <td>OSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f39zg</td>\n",
       "      <td>[Michael VanderHeijden]</td>\n",
       "      <td>[Law, Legal Writing and Research]</td>\n",
       "      <td>How Little Is Known: Finding Regulations from ...</td>\n",
       "      <td>The task of finding nineteenth century regulat...</td>\n",
       "      <td>https://osf.io/preprints/lawarchive/f39zg/</td>\n",
       "      <td>2023-09-22T15:32:37.192989</td>\n",
       "      <td>[michael.vanderheijden@yale.edu]</td>\n",
       "      <td>OSF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>2308.06220</td>\n",
       "      <td>[Noah D. Gade, Jordan Rodu]</td>\n",
       "      <td>Methodology (stat.ME); Machine Learning (stat.ML)</td>\n",
       "      <td>Nonlinear Permuted Granger Causality</td>\n",
       "      <td>Granger causal inference is a contentious but ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>2309.06305</td>\n",
       "      <td>[Jacob Dorn, Luther Yap]</td>\n",
       "      <td>Econometrics (econ.EM); Methodology (stat.ME)</td>\n",
       "      <td>Sensitivity Analysis for Linear Estimands</td>\n",
       "      <td>We propose a novel sensitivity analysis framew...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>2309.07180</td>\n",
       "      <td>[Jennifer Park, Rochelle E. Tractenberg]</td>\n",
       "      <td>Other Statistics (stat.OT); Applications (stat...</td>\n",
       "      <td>How do ASA Ethical Guidelines Support U.S. Gui...</td>\n",
       "      <td>In 2022, the American Statistical Association ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2203.01435</td>\n",
       "      <td>[František Bartoš, Eric-Jan Wagenmakers]</td>\n",
       "      <td>Methodology (stat.ME)</td>\n",
       "      <td>A general approximation to nested Bayes factor...</td>\n",
       "      <td>A staple of Bayesian model comparison and hypo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>2302.00774</td>\n",
       "      <td>[Ulrich Schimmack, František Bartoš]</td>\n",
       "      <td>Applications (stat.AP); Methodology (stat.ME)</td>\n",
       "      <td>Estimating the false discovery risk of (random...</td>\n",
       "      <td>The influential claim that most published resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4948 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id                                            authors  \\\n",
       "0          k5ghb                                  [Setblon Tembang]   \n",
       "1          yjga5  [Emma Thornton, Kimberly Petersen, Jose Marque...   \n",
       "2          8ptdg  [Jean-François Bonnefon, Iyad Rahwan, Azim Sha...   \n",
       "3          hk7fp     [Jordan D Klein, Anjarasoa Rasoanomenjanahary]   \n",
       "4          f39zg                            [Michael VanderHeijden]   \n",
       "...          ...                                                ...   \n",
       "1539  2308.06220                        [Noah D. Gade, Jordan Rodu]   \n",
       "1545  2309.06305                           [Jacob Dorn, Luther Yap]   \n",
       "1546  2309.07180           [Jennifer Park, Rochelle E. Tractenberg]   \n",
       "1550  2203.01435           [František Bartoš, Eric-Jan Wagenmakers]   \n",
       "1554  2302.00774               [Ulrich Schimmack, František Bartoš]   \n",
       "\n",
       "                                               subjects  \\\n",
       "0     [Social and Behavioral Sciences, Psychology, O...   \n",
       "1                      [Social and Behavioral Sciences]   \n",
       "2                      [Social and Behavioral Sciences]   \n",
       "3     [Medicine and Health Sciences, Public Health, ...   \n",
       "4                     [Law, Legal Writing and Research]   \n",
       "...                                                 ...   \n",
       "1539  Methodology (stat.ME); Machine Learning (stat.ML)   \n",
       "1545      Econometrics (econ.EM); Methodology (stat.ME)   \n",
       "1546  Other Statistics (stat.OT); Applications (stat...   \n",
       "1550                              Methodology (stat.ME)   \n",
       "1554      Applications (stat.AP); Methodology (stat.ME)   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Pentingnya Konseling Eksistensial-Humanistik d...   \n",
       "1     Do patterns of adolescent participation in art...   \n",
       "2       The moral psychology of Artificial Intelligence   \n",
       "3     Climate Change and Health Transitions: Evidenc...   \n",
       "4     How Little Is Known: Finding Regulations from ...   \n",
       "...                                                 ...   \n",
       "1539               Nonlinear Permuted Granger Causality   \n",
       "1545          Sensitivity Analysis for Linear Estimands   \n",
       "1546  How do ASA Ethical Guidelines Support U.S. Gui...   \n",
       "1550  A general approximation to nested Bayes factor...   \n",
       "1554  Estimating the false discovery risk of (random...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Penelitian ini merupakan penelitian kulitatif ...   \n",
       "1     Using latent class analysis, we sought to esta...   \n",
       "2     Moral psychology was shaped around three categ...   \n",
       "3     BACKGROUND\\nGlobal climate change poses grave ...   \n",
       "4     The task of finding nineteenth century regulat...   \n",
       "...                                                 ...   \n",
       "1539  Granger causal inference is a contentious but ...   \n",
       "1545  We propose a novel sensitivity analysis framew...   \n",
       "1546  In 2022, the American Statistical Association ...   \n",
       "1550  A staple of Bayesian model comparison and hypo...   \n",
       "1554  The influential claim that most published resu...   \n",
       "\n",
       "                                             url                        date  \\\n",
       "0                          https://osf.io/k5ghb/  2023-09-22T15:11:44.880299   \n",
       "1                    https://psyarxiv.com/yjga5/  2023-09-22T15:12:25.767233   \n",
       "2                    https://psyarxiv.com/8ptdg/  2023-09-22T15:10:52.677112   \n",
       "3                          https://osf.io/hk7fp/  2023-09-22T14:32:27.106595   \n",
       "4     https://osf.io/preprints/lawarchive/f39zg/  2023-09-22T15:32:37.192989   \n",
       "...                                          ...                         ...   \n",
       "1539                                         NaN                  2023-09-19   \n",
       "1545                                         NaN                  2023-09-19   \n",
       "1546                                         NaN                  2023-09-19   \n",
       "1550                                         NaN                  2023-09-19   \n",
       "1554                                         NaN                  2023-09-19   \n",
       "\n",
       "                                           affiliations source  \n",
       "0                                                    []    OSF  \n",
       "1                                                    []    OSF  \n",
       "2                [jean-francois.bonnefon@tse-fr.eu, VT]    OSF  \n",
       "3     [jdklein@princeton.edu, OU, UND , IC, anjaraso...    OSF  \n",
       "4                      [michael.vanderheijden@yale.edu]    OSF  \n",
       "...                                                 ...    ...  \n",
       "1539                                                 []  arXiv  \n",
       "1545                                                 []  arXiv  \n",
       "1546                                                 []  arXiv  \n",
       "1550                                                 []  arXiv  \n",
       "1554                                                 []  arXiv  \n",
       "\n",
       "[4948 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['University of Virginia', 'ndge@virginia.edu', 'jsrq@virginia.edu']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_arxiv_old('2308.06220')# this checks that selenium works on a given machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "path = r'C:\\Users\\ethan/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "def sentenceSimplifier(txt): \n",
    "    doc = nlp(txt)\n",
    "    out = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['PROPN', 'VERB','NUM', 'NOUN']:\n",
    "            out+=token.lemma_+\" \"\n",
    "    return out[:-1]\n",
    "def sentence2vec(txt):\n",
    "    simplified_txt = sentenceSimplifier(txt)\n",
    "    words = simplified_txt.split(\" \")\n",
    "    vecs_arr = np.array([model[word] for word in words if word in model]).sum(axis=0)/len(words)\n",
    "    return  vecs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\swifter\\swifter.py:87: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cfff3580704fb194e80f32197bda04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/4948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>subjects</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>source</th>\n",
       "      <th>abstract_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k5ghb</td>\n",
       "      <td>[Setblon Tembang]</td>\n",
       "      <td>[Social and Behavioral Sciences, Psychology, O...</td>\n",
       "      <td>Pentingnya Konseling Eksistensial-Humanistik d...</td>\n",
       "      <td>Penelitian ini merupakan penelitian kulitatif ...</td>\n",
       "      <td>https://osf.io/k5ghb/</td>\n",
       "      <td>2023-09-22T15:11:44.880299</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.054275375, -0.0053556077, 0.021748861, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yjga5</td>\n",
       "      <td>[Emma Thornton, Kimberly Petersen, Jose Marque...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>Do patterns of adolescent participation in art...</td>\n",
       "      <td>Using latent class analysis, we sought to esta...</td>\n",
       "      <td>https://psyarxiv.com/yjga5/</td>\n",
       "      <td>2023-09-22T15:12:25.767233</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.013113845, -0.050486002, -0.029627712, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ptdg</td>\n",
       "      <td>[Jean-François Bonnefon, Iyad Rahwan, Azim Sha...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>The moral psychology of Artificial Intelligence</td>\n",
       "      <td>Moral psychology was shaped around three categ...</td>\n",
       "      <td>https://psyarxiv.com/8ptdg/</td>\n",
       "      <td>2023-09-22T15:10:52.677112</td>\n",
       "      <td>[jean-francois.bonnefon@tse-fr.eu, VT]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.042172242, 0.04809367, -0.01174647, 0.05990...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hk7fp</td>\n",
       "      <td>[Jordan D Klein, Anjarasoa Rasoanomenjanahary]</td>\n",
       "      <td>[Medicine and Health Sciences, Public Health, ...</td>\n",
       "      <td>Climate Change and Health Transitions: Evidenc...</td>\n",
       "      <td>BACKGROUND\\nGlobal climate change poses grave ...</td>\n",
       "      <td>https://osf.io/hk7fp/</td>\n",
       "      <td>2023-09-22T14:32:27.106595</td>\n",
       "      <td>[jdklein@princeton.edu, OU, UND , IC, anjaraso...</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.0114671495, 0.09863281, -0.040132143, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f39zg</td>\n",
       "      <td>[Michael VanderHeijden]</td>\n",
       "      <td>[Law, Legal Writing and Research]</td>\n",
       "      <td>How Little Is Known: Finding Regulations from ...</td>\n",
       "      <td>The task of finding nineteenth century regulat...</td>\n",
       "      <td>https://osf.io/preprints/lawarchive/f39zg/</td>\n",
       "      <td>2023-09-22T15:32:37.192989</td>\n",
       "      <td>[michael.vanderheijden@yale.edu]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.0060602822, 0.012296041, -0.018091837, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>2308.06220</td>\n",
       "      <td>[Noah D. Gade, Jordan Rodu]</td>\n",
       "      <td>Methodology (stat.ME); Machine Learning (stat.ML)</td>\n",
       "      <td>Nonlinear Permuted Granger Causality</td>\n",
       "      <td>Granger causal inference is a contentious but ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[0.017944157, -0.013425849, 0.02205308, 0.0719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>2309.06305</td>\n",
       "      <td>[Jacob Dorn, Luther Yap]</td>\n",
       "      <td>Econometrics (econ.EM); Methodology (stat.ME)</td>\n",
       "      <td>Sensitivity Analysis for Linear Estimands</td>\n",
       "      <td>We propose a novel sensitivity analysis framew...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.004429767, -0.0180908, 0.005612825, 0.0742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>2309.07180</td>\n",
       "      <td>[Jennifer Park, Rochelle E. Tractenberg]</td>\n",
       "      <td>Other Statistics (stat.OT); Applications (stat...</td>\n",
       "      <td>How do ASA Ethical Guidelines Support U.S. Gui...</td>\n",
       "      <td>In 2022, the American Statistical Association ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.056426037, -0.045546602, 0.039102975, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2203.01435</td>\n",
       "      <td>[František Bartoš, Eric-Jan Wagenmakers]</td>\n",
       "      <td>Methodology (stat.ME)</td>\n",
       "      <td>A general approximation to nested Bayes factor...</td>\n",
       "      <td>A staple of Bayesian model comparison and hypo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[0.028341988, -0.018754406, -0.007142254, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>2302.00774</td>\n",
       "      <td>[Ulrich Schimmack, František Bartoš]</td>\n",
       "      <td>Applications (stat.AP); Methodology (stat.ME)</td>\n",
       "      <td>Estimating the false discovery risk of (random...</td>\n",
       "      <td>The influential claim that most published resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.03631415, -0.016391208, -0.0022354126, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4948 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id                                            authors  \\\n",
       "0          k5ghb                                  [Setblon Tembang]   \n",
       "1          yjga5  [Emma Thornton, Kimberly Petersen, Jose Marque...   \n",
       "2          8ptdg  [Jean-François Bonnefon, Iyad Rahwan, Azim Sha...   \n",
       "3          hk7fp     [Jordan D Klein, Anjarasoa Rasoanomenjanahary]   \n",
       "4          f39zg                            [Michael VanderHeijden]   \n",
       "...          ...                                                ...   \n",
       "1539  2308.06220                        [Noah D. Gade, Jordan Rodu]   \n",
       "1545  2309.06305                           [Jacob Dorn, Luther Yap]   \n",
       "1546  2309.07180           [Jennifer Park, Rochelle E. Tractenberg]   \n",
       "1550  2203.01435           [František Bartoš, Eric-Jan Wagenmakers]   \n",
       "1554  2302.00774               [Ulrich Schimmack, František Bartoš]   \n",
       "\n",
       "                                               subjects  \\\n",
       "0     [Social and Behavioral Sciences, Psychology, O...   \n",
       "1                      [Social and Behavioral Sciences]   \n",
       "2                      [Social and Behavioral Sciences]   \n",
       "3     [Medicine and Health Sciences, Public Health, ...   \n",
       "4                     [Law, Legal Writing and Research]   \n",
       "...                                                 ...   \n",
       "1539  Methodology (stat.ME); Machine Learning (stat.ML)   \n",
       "1545      Econometrics (econ.EM); Methodology (stat.ME)   \n",
       "1546  Other Statistics (stat.OT); Applications (stat...   \n",
       "1550                              Methodology (stat.ME)   \n",
       "1554      Applications (stat.AP); Methodology (stat.ME)   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Pentingnya Konseling Eksistensial-Humanistik d...   \n",
       "1     Do patterns of adolescent participation in art...   \n",
       "2       The moral psychology of Artificial Intelligence   \n",
       "3     Climate Change and Health Transitions: Evidenc...   \n",
       "4     How Little Is Known: Finding Regulations from ...   \n",
       "...                                                 ...   \n",
       "1539               Nonlinear Permuted Granger Causality   \n",
       "1545          Sensitivity Analysis for Linear Estimands   \n",
       "1546  How do ASA Ethical Guidelines Support U.S. Gui...   \n",
       "1550  A general approximation to nested Bayes factor...   \n",
       "1554  Estimating the false discovery risk of (random...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Penelitian ini merupakan penelitian kulitatif ...   \n",
       "1     Using latent class analysis, we sought to esta...   \n",
       "2     Moral psychology was shaped around three categ...   \n",
       "3     BACKGROUND\\nGlobal climate change poses grave ...   \n",
       "4     The task of finding nineteenth century regulat...   \n",
       "...                                                 ...   \n",
       "1539  Granger causal inference is a contentious but ...   \n",
       "1545  We propose a novel sensitivity analysis framew...   \n",
       "1546  In 2022, the American Statistical Association ...   \n",
       "1550  A staple of Bayesian model comparison and hypo...   \n",
       "1554  The influential claim that most published resu...   \n",
       "\n",
       "                                             url                        date  \\\n",
       "0                          https://osf.io/k5ghb/  2023-09-22T15:11:44.880299   \n",
       "1                    https://psyarxiv.com/yjga5/  2023-09-22T15:12:25.767233   \n",
       "2                    https://psyarxiv.com/8ptdg/  2023-09-22T15:10:52.677112   \n",
       "3                          https://osf.io/hk7fp/  2023-09-22T14:32:27.106595   \n",
       "4     https://osf.io/preprints/lawarchive/f39zg/  2023-09-22T15:32:37.192989   \n",
       "...                                          ...                         ...   \n",
       "1539                                         NaN                  2023-09-19   \n",
       "1545                                         NaN                  2023-09-19   \n",
       "1546                                         NaN                  2023-09-19   \n",
       "1550                                         NaN                  2023-09-19   \n",
       "1554                                         NaN                  2023-09-19   \n",
       "\n",
       "                                           affiliations source  \\\n",
       "0                                                    []    OSF   \n",
       "1                                                    []    OSF   \n",
       "2                [jean-francois.bonnefon@tse-fr.eu, VT]    OSF   \n",
       "3     [jdklein@princeton.edu, OU, UND , IC, anjaraso...    OSF   \n",
       "4                      [michael.vanderheijden@yale.edu]    OSF   \n",
       "...                                                 ...    ...   \n",
       "1539                                                 []  arXiv   \n",
       "1545                                                 []  arXiv   \n",
       "1546                                                 []  arXiv   \n",
       "1550                                                 []  arXiv   \n",
       "1554                                                 []  arXiv   \n",
       "\n",
       "                                           abstract_vec  \n",
       "0     [0.054275375, -0.0053556077, 0.021748861, 0.02...  \n",
       "1     [0.013113845, -0.050486002, -0.029627712, 0.06...  \n",
       "2     [0.042172242, 0.04809367, -0.01174647, 0.05990...  \n",
       "3     [-0.0114671495, 0.09863281, -0.040132143, 0.10...  \n",
       "4     [-0.0060602822, 0.012296041, -0.018091837, 0.0...  \n",
       "...                                                 ...  \n",
       "1539  [0.017944157, -0.013425849, 0.02205308, 0.0719...  \n",
       "1545  [-0.004429767, -0.0180908, 0.005612825, 0.0742...  \n",
       "1546  [-0.056426037, -0.045546602, 0.039102975, 0.08...  \n",
       "1550  [0.028341988, -0.018754406, -0.007142254, 0.06...  \n",
       "1554  [-0.03631415, -0.016391208, -0.0022354126, 0.0...  \n",
       "\n",
       "[4948 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['abstract_vec'] = df2['abstract'].apply(sentence2vec)\n",
    "df2.to_pickle('de_duped_w_vec.pkl')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>subjects</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>source</th>\n",
       "      <th>abstract_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k5ghb</td>\n",
       "      <td>[Setblon Tembang]</td>\n",
       "      <td>[Social and Behavioral Sciences, Psychology, O...</td>\n",
       "      <td>Pentingnya Konseling Eksistensial-Humanistik d...</td>\n",
       "      <td>Penelitian ini merupakan penelitian kulitatif ...</td>\n",
       "      <td>https://osf.io/k5ghb/</td>\n",
       "      <td>2023-09-22T15:11:44.880299</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.054275375, -0.0053556077, 0.021748861, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yjga5</td>\n",
       "      <td>[Emma Thornton, Kimberly Petersen, Jose Marque...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>Do patterns of adolescent participation in art...</td>\n",
       "      <td>Using latent class analysis, we sought to esta...</td>\n",
       "      <td>https://psyarxiv.com/yjga5/</td>\n",
       "      <td>2023-09-22T15:12:25.767233</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.013113845, -0.050486002, -0.029627712, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ptdg</td>\n",
       "      <td>[Jean-François Bonnefon, Iyad Rahwan, Azim Sha...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>The moral psychology of Artificial Intelligence</td>\n",
       "      <td>Moral psychology was shaped around three categ...</td>\n",
       "      <td>https://psyarxiv.com/8ptdg/</td>\n",
       "      <td>2023-09-22T15:10:52.677112</td>\n",
       "      <td>[jean-francois.bonnefon@tse-fr.eu, VT]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.042172242, 0.04809367, -0.01174647, 0.05990...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hk7fp</td>\n",
       "      <td>[Jordan D Klein, Anjarasoa Rasoanomenjanahary]</td>\n",
       "      <td>[Medicine and Health Sciences, Public Health, ...</td>\n",
       "      <td>Climate Change and Health Transitions: Evidenc...</td>\n",
       "      <td>BACKGROUND\\nGlobal climate change poses grave ...</td>\n",
       "      <td>https://osf.io/hk7fp/</td>\n",
       "      <td>2023-09-22T14:32:27.106595</td>\n",
       "      <td>[jdklein@princeton.edu, OU, UND , IC, anjaraso...</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.0114671495, 0.09863281, -0.040132143, 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f39zg</td>\n",
       "      <td>[Michael VanderHeijden]</td>\n",
       "      <td>[Law, Legal Writing and Research]</td>\n",
       "      <td>How Little Is Known: Finding Regulations from ...</td>\n",
       "      <td>The task of finding nineteenth century regulat...</td>\n",
       "      <td>https://osf.io/preprints/lawarchive/f39zg/</td>\n",
       "      <td>2023-09-22T15:32:37.192989</td>\n",
       "      <td>[michael.vanderheijden@yale.edu]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.0060602822, 0.012296041, -0.018091837, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>2308.06220</td>\n",
       "      <td>[Noah D. Gade, Jordan Rodu]</td>\n",
       "      <td>Methodology (stat.ME); Machine Learning (stat.ML)</td>\n",
       "      <td>Nonlinear Permuted Granger Causality</td>\n",
       "      <td>Granger causal inference is a contentious but ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[0.017944157, -0.013425849, 0.02205308, 0.0719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>2309.06305</td>\n",
       "      <td>[Jacob Dorn, Luther Yap]</td>\n",
       "      <td>Econometrics (econ.EM); Methodology (stat.ME)</td>\n",
       "      <td>Sensitivity Analysis for Linear Estimands</td>\n",
       "      <td>We propose a novel sensitivity analysis framew...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.004429767, -0.0180908, 0.005612825, 0.0742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>2309.07180</td>\n",
       "      <td>[Jennifer Park, Rochelle E. Tractenberg]</td>\n",
       "      <td>Other Statistics (stat.OT); Applications (stat...</td>\n",
       "      <td>How do ASA Ethical Guidelines Support U.S. Gui...</td>\n",
       "      <td>In 2022, the American Statistical Association ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.056426037, -0.045546602, 0.039102975, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2203.01435</td>\n",
       "      <td>[František Bartoš, Eric-Jan Wagenmakers]</td>\n",
       "      <td>Methodology (stat.ME)</td>\n",
       "      <td>A general approximation to nested Bayes factor...</td>\n",
       "      <td>A staple of Bayesian model comparison and hypo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[0.028341988, -0.018754406, -0.007142254, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>2302.00774</td>\n",
       "      <td>[Ulrich Schimmack, František Bartoš]</td>\n",
       "      <td>Applications (stat.AP); Methodology (stat.ME)</td>\n",
       "      <td>Estimating the false discovery risk of (random...</td>\n",
       "      <td>The influential claim that most published resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.03631415, -0.016391208, -0.0022354126, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4948 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id                                            authors  \\\n",
       "0          k5ghb                                  [Setblon Tembang]   \n",
       "1          yjga5  [Emma Thornton, Kimberly Petersen, Jose Marque...   \n",
       "2          8ptdg  [Jean-François Bonnefon, Iyad Rahwan, Azim Sha...   \n",
       "3          hk7fp     [Jordan D Klein, Anjarasoa Rasoanomenjanahary]   \n",
       "4          f39zg                            [Michael VanderHeijden]   \n",
       "...          ...                                                ...   \n",
       "1539  2308.06220                        [Noah D. Gade, Jordan Rodu]   \n",
       "1545  2309.06305                           [Jacob Dorn, Luther Yap]   \n",
       "1546  2309.07180           [Jennifer Park, Rochelle E. Tractenberg]   \n",
       "1550  2203.01435           [František Bartoš, Eric-Jan Wagenmakers]   \n",
       "1554  2302.00774               [Ulrich Schimmack, František Bartoš]   \n",
       "\n",
       "                                               subjects  \\\n",
       "0     [Social and Behavioral Sciences, Psychology, O...   \n",
       "1                      [Social and Behavioral Sciences]   \n",
       "2                      [Social and Behavioral Sciences]   \n",
       "3     [Medicine and Health Sciences, Public Health, ...   \n",
       "4                     [Law, Legal Writing and Research]   \n",
       "...                                                 ...   \n",
       "1539  Methodology (stat.ME); Machine Learning (stat.ML)   \n",
       "1545      Econometrics (econ.EM); Methodology (stat.ME)   \n",
       "1546  Other Statistics (stat.OT); Applications (stat...   \n",
       "1550                              Methodology (stat.ME)   \n",
       "1554      Applications (stat.AP); Methodology (stat.ME)   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Pentingnya Konseling Eksistensial-Humanistik d...   \n",
       "1     Do patterns of adolescent participation in art...   \n",
       "2       The moral psychology of Artificial Intelligence   \n",
       "3     Climate Change and Health Transitions: Evidenc...   \n",
       "4     How Little Is Known: Finding Regulations from ...   \n",
       "...                                                 ...   \n",
       "1539               Nonlinear Permuted Granger Causality   \n",
       "1545          Sensitivity Analysis for Linear Estimands   \n",
       "1546  How do ASA Ethical Guidelines Support U.S. Gui...   \n",
       "1550  A general approximation to nested Bayes factor...   \n",
       "1554  Estimating the false discovery risk of (random...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Penelitian ini merupakan penelitian kulitatif ...   \n",
       "1     Using latent class analysis, we sought to esta...   \n",
       "2     Moral psychology was shaped around three categ...   \n",
       "3     BACKGROUND\\nGlobal climate change poses grave ...   \n",
       "4     The task of finding nineteenth century regulat...   \n",
       "...                                                 ...   \n",
       "1539  Granger causal inference is a contentious but ...   \n",
       "1545  We propose a novel sensitivity analysis framew...   \n",
       "1546  In 2022, the American Statistical Association ...   \n",
       "1550  A staple of Bayesian model comparison and hypo...   \n",
       "1554  The influential claim that most published resu...   \n",
       "\n",
       "                                             url                        date  \\\n",
       "0                          https://osf.io/k5ghb/  2023-09-22T15:11:44.880299   \n",
       "1                    https://psyarxiv.com/yjga5/  2023-09-22T15:12:25.767233   \n",
       "2                    https://psyarxiv.com/8ptdg/  2023-09-22T15:10:52.677112   \n",
       "3                          https://osf.io/hk7fp/  2023-09-22T14:32:27.106595   \n",
       "4     https://osf.io/preprints/lawarchive/f39zg/  2023-09-22T15:32:37.192989   \n",
       "...                                          ...                         ...   \n",
       "1539                                         NaN                  2023-09-19   \n",
       "1545                                         NaN                  2023-09-19   \n",
       "1546                                         NaN                  2023-09-19   \n",
       "1550                                         NaN                  2023-09-19   \n",
       "1554                                         NaN                  2023-09-19   \n",
       "\n",
       "                                           affiliations source  \\\n",
       "0                                                    []    OSF   \n",
       "1                                                    []    OSF   \n",
       "2                [jean-francois.bonnefon@tse-fr.eu, VT]    OSF   \n",
       "3     [jdklein@princeton.edu, OU, UND , IC, anjaraso...    OSF   \n",
       "4                      [michael.vanderheijden@yale.edu]    OSF   \n",
       "...                                                 ...    ...   \n",
       "1539                                                 []  arXiv   \n",
       "1545                                                 []  arXiv   \n",
       "1546                                                 []  arXiv   \n",
       "1550                                                 []  arXiv   \n",
       "1554                                                 []  arXiv   \n",
       "\n",
       "                                           abstract_vec  \n",
       "0     [0.054275375, -0.0053556077, 0.021748861, 0.02...  \n",
       "1     [0.013113845, -0.050486002, -0.029627712, 0.06...  \n",
       "2     [0.042172242, 0.04809367, -0.01174647, 0.05990...  \n",
       "3     [-0.0114671495, 0.09863281, -0.040132143, 0.10...  \n",
       "4     [-0.0060602822, 0.012296041, -0.018091837, 0.0...  \n",
       "...                                                 ...  \n",
       "1539  [0.017944157, -0.013425849, 0.02205308, 0.0719...  \n",
       "1545  [-0.004429767, -0.0180908, 0.005612825, 0.0742...  \n",
       "1546  [-0.056426037, -0.045546602, 0.039102975, 0.08...  \n",
       "1550  [0.028341988, -0.018754406, -0.007142254, 0.06...  \n",
       "1554  [-0.03631415, -0.016391208, -0.0022354126, 0.0...  \n",
       "\n",
       "[4948 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.read_pickle('de_duped_w_vec.pkl')\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>subjects</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>source</th>\n",
       "      <th>abstract_vec</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yjga5</td>\n",
       "      <td>[Emma Thornton, Kimberly Petersen, Jose Marque...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>Do patterns of adolescent participation in art...</td>\n",
       "      <td>Using latent class analysis, we sought to esta...</td>\n",
       "      <td>https://psyarxiv.com/yjga5/</td>\n",
       "      <td>2023-09-22T15:12:25.767233</td>\n",
       "      <td>[]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.013113845, -0.050486002, -0.029627712, 0.06...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8ptdg</td>\n",
       "      <td>[Jean-François Bonnefon, Iyad Rahwan, Azim Sha...</td>\n",
       "      <td>[Social and Behavioral Sciences]</td>\n",
       "      <td>The moral psychology of Artificial Intelligence</td>\n",
       "      <td>Moral psychology was shaped around three categ...</td>\n",
       "      <td>https://psyarxiv.com/8ptdg/</td>\n",
       "      <td>2023-09-22T15:10:52.677112</td>\n",
       "      <td>[jean-francois.bonnefon@tse-fr.eu, VT]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[0.042172242, 0.04809367, -0.01174647, 0.05990...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hk7fp</td>\n",
       "      <td>[Jordan D Klein, Anjarasoa Rasoanomenjanahary]</td>\n",
       "      <td>[Medicine and Health Sciences, Public Health, ...</td>\n",
       "      <td>Climate Change and Health Transitions: Evidenc...</td>\n",
       "      <td>BACKGROUND\\nGlobal climate change poses grave ...</td>\n",
       "      <td>https://osf.io/hk7fp/</td>\n",
       "      <td>2023-09-22T14:32:27.106595</td>\n",
       "      <td>[jdklein@princeton.edu, OU, UND , IC, anjaraso...</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.0114671495, 0.09863281, -0.040132143, 0.10...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f39zg</td>\n",
       "      <td>[Michael VanderHeijden]</td>\n",
       "      <td>[Law, Legal Writing and Research]</td>\n",
       "      <td>How Little Is Known: Finding Regulations from ...</td>\n",
       "      <td>The task of finding nineteenth century regulat...</td>\n",
       "      <td>https://osf.io/preprints/lawarchive/f39zg/</td>\n",
       "      <td>2023-09-22T15:32:37.192989</td>\n",
       "      <td>[michael.vanderheijden@yale.edu]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.0060602822, 0.012296041, -0.018091837, 0.0...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mkszn</td>\n",
       "      <td>[Shahan Mamoor]</td>\n",
       "      <td>[Life Sciences, Genetics and Genomics, Genomics]</td>\n",
       "      <td>WDR45B is differentially expressed in central ...</td>\n",
       "      <td>Metastasis to the brain occurs in a significan...</td>\n",
       "      <td>https://osf.io/mkszn/</td>\n",
       "      <td>2023-09-22T14:20:59.006294</td>\n",
       "      <td>[shahanmamoor@gmail.com, USA]</td>\n",
       "      <td>OSF</td>\n",
       "      <td>[-0.015696138, 0.116664164, 0.048419025, 0.048...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>2308.06220</td>\n",
       "      <td>[Noah D. Gade, Jordan Rodu]</td>\n",
       "      <td>Methodology (stat.ME); Machine Learning (stat.ML)</td>\n",
       "      <td>Nonlinear Permuted Granger Causality</td>\n",
       "      <td>Granger causal inference is a contentious but ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[0.017944157, -0.013425849, 0.02205308, 0.0719...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>2309.06305</td>\n",
       "      <td>[Jacob Dorn, Luther Yap]</td>\n",
       "      <td>Econometrics (econ.EM); Methodology (stat.ME)</td>\n",
       "      <td>Sensitivity Analysis for Linear Estimands</td>\n",
       "      <td>We propose a novel sensitivity analysis framew...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.004429767, -0.0180908, 0.005612825, 0.0742...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>2309.07180</td>\n",
       "      <td>[Jennifer Park, Rochelle E. Tractenberg]</td>\n",
       "      <td>Other Statistics (stat.OT); Applications (stat...</td>\n",
       "      <td>How do ASA Ethical Guidelines Support U.S. Gui...</td>\n",
       "      <td>In 2022, the American Statistical Association ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.056426037, -0.045546602, 0.039102975, 0.08...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>2203.01435</td>\n",
       "      <td>[František Bartoš, Eric-Jan Wagenmakers]</td>\n",
       "      <td>Methodology (stat.ME)</td>\n",
       "      <td>A general approximation to nested Bayes factor...</td>\n",
       "      <td>A staple of Bayesian model comparison and hypo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[0.028341988, -0.018754406, -0.007142254, 0.06...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>2302.00774</td>\n",
       "      <td>[Ulrich Schimmack, František Bartoš]</td>\n",
       "      <td>Applications (stat.AP); Methodology (stat.ME)</td>\n",
       "      <td>Estimating the false discovery risk of (random...</td>\n",
       "      <td>The influential claim that most published resu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>[-0.03631415, -0.016391208, -0.0022354126, 0.0...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4815 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        paper_id                                            authors  \\\n",
       "1          yjga5  [Emma Thornton, Kimberly Petersen, Jose Marque...   \n",
       "2          8ptdg  [Jean-François Bonnefon, Iyad Rahwan, Azim Sha...   \n",
       "3          hk7fp     [Jordan D Klein, Anjarasoa Rasoanomenjanahary]   \n",
       "4          f39zg                            [Michael VanderHeijden]   \n",
       "5          mkszn                                    [Shahan Mamoor]   \n",
       "...          ...                                                ...   \n",
       "1539  2308.06220                        [Noah D. Gade, Jordan Rodu]   \n",
       "1545  2309.06305                           [Jacob Dorn, Luther Yap]   \n",
       "1546  2309.07180           [Jennifer Park, Rochelle E. Tractenberg]   \n",
       "1550  2203.01435           [František Bartoš, Eric-Jan Wagenmakers]   \n",
       "1554  2302.00774               [Ulrich Schimmack, František Bartoš]   \n",
       "\n",
       "                                               subjects  \\\n",
       "1                      [Social and Behavioral Sciences]   \n",
       "2                      [Social and Behavioral Sciences]   \n",
       "3     [Medicine and Health Sciences, Public Health, ...   \n",
       "4                     [Law, Legal Writing and Research]   \n",
       "5      [Life Sciences, Genetics and Genomics, Genomics]   \n",
       "...                                                 ...   \n",
       "1539  Methodology (stat.ME); Machine Learning (stat.ML)   \n",
       "1545      Econometrics (econ.EM); Methodology (stat.ME)   \n",
       "1546  Other Statistics (stat.OT); Applications (stat...   \n",
       "1550                              Methodology (stat.ME)   \n",
       "1554      Applications (stat.AP); Methodology (stat.ME)   \n",
       "\n",
       "                                                  title  \\\n",
       "1     Do patterns of adolescent participation in art...   \n",
       "2       The moral psychology of Artificial Intelligence   \n",
       "3     Climate Change and Health Transitions: Evidenc...   \n",
       "4     How Little Is Known: Finding Regulations from ...   \n",
       "5     WDR45B is differentially expressed in central ...   \n",
       "...                                                 ...   \n",
       "1539               Nonlinear Permuted Granger Causality   \n",
       "1545          Sensitivity Analysis for Linear Estimands   \n",
       "1546  How do ASA Ethical Guidelines Support U.S. Gui...   \n",
       "1550  A general approximation to nested Bayes factor...   \n",
       "1554  Estimating the false discovery risk of (random...   \n",
       "\n",
       "                                               abstract  \\\n",
       "1     Using latent class analysis, we sought to esta...   \n",
       "2     Moral psychology was shaped around three categ...   \n",
       "3     BACKGROUND\\nGlobal climate change poses grave ...   \n",
       "4     The task of finding nineteenth century regulat...   \n",
       "5     Metastasis to the brain occurs in a significan...   \n",
       "...                                                 ...   \n",
       "1539  Granger causal inference is a contentious but ...   \n",
       "1545  We propose a novel sensitivity analysis framew...   \n",
       "1546  In 2022, the American Statistical Association ...   \n",
       "1550  A staple of Bayesian model comparison and hypo...   \n",
       "1554  The influential claim that most published resu...   \n",
       "\n",
       "                                             url                        date  \\\n",
       "1                    https://psyarxiv.com/yjga5/  2023-09-22T15:12:25.767233   \n",
       "2                    https://psyarxiv.com/8ptdg/  2023-09-22T15:10:52.677112   \n",
       "3                          https://osf.io/hk7fp/  2023-09-22T14:32:27.106595   \n",
       "4     https://osf.io/preprints/lawarchive/f39zg/  2023-09-22T15:32:37.192989   \n",
       "5                          https://osf.io/mkszn/  2023-09-22T14:20:59.006294   \n",
       "...                                          ...                         ...   \n",
       "1539                                         NaN                  2023-09-19   \n",
       "1545                                         NaN                  2023-09-19   \n",
       "1546                                         NaN                  2023-09-19   \n",
       "1550                                         NaN                  2023-09-19   \n",
       "1554                                         NaN                  2023-09-19   \n",
       "\n",
       "                                           affiliations source  \\\n",
       "1                                                    []    OSF   \n",
       "2                [jean-francois.bonnefon@tse-fr.eu, VT]    OSF   \n",
       "3     [jdklein@princeton.edu, OU, UND , IC, anjaraso...    OSF   \n",
       "4                      [michael.vanderheijden@yale.edu]    OSF   \n",
       "5                         [shahanmamoor@gmail.com, USA]    OSF   \n",
       "...                                                 ...    ...   \n",
       "1539                                                 []  arXiv   \n",
       "1545                                                 []  arXiv   \n",
       "1546                                                 []  arXiv   \n",
       "1550                                                 []  arXiv   \n",
       "1554                                                 []  arXiv   \n",
       "\n",
       "                                           abstract_vec language  \n",
       "1     [0.013113845, -0.050486002, -0.029627712, 0.06...       en  \n",
       "2     [0.042172242, 0.04809367, -0.01174647, 0.05990...       en  \n",
       "3     [-0.0114671495, 0.09863281, -0.040132143, 0.10...       en  \n",
       "4     [-0.0060602822, 0.012296041, -0.018091837, 0.0...       en  \n",
       "5     [-0.015696138, 0.116664164, 0.048419025, 0.048...       en  \n",
       "...                                                 ...      ...  \n",
       "1539  [0.017944157, -0.013425849, 0.02205308, 0.0719...       en  \n",
       "1545  [-0.004429767, -0.0180908, 0.005612825, 0.0742...       en  \n",
       "1546  [-0.056426037, -0.045546602, 0.039102975, 0.08...       en  \n",
       "1550  [0.028341988, -0.018754406, -0.007142254, 0.06...       en  \n",
       "1554  [-0.03631415, -0.016391208, -0.0022354126, 0.0...       en  \n",
       "\n",
       "[4815 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "def get_lang(x):\n",
    "    try:\n",
    "        return detect(x)\n",
    "    except:\n",
    "        return 'Fail'\n",
    "df5['language'] = df5['abstract'].apply(get_lang)\n",
    "df6 = df5[df5['language'] == 'en']\n",
    "df6.to_pickle('de_duped_w_vec_en.pkl')\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=cs&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=math&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=q-bio&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=hep-th&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=stat&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=econ&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=eess&method=with&syear=2023\n",
      "https://arxiv.org/catchup?smonth=9&group=grp_&sday=21&archive=q-fin&method=with&syear=2023\n"
     ]
    }
   ],
   "source": [
    "for subject in ['cs', 'math', 'q-bio', 'hep-th', 'stat', 'econ', 'eess', 'q-fin']:\n",
    "    url = f\"https://arxiv.org/catchup?smonth={str(date.month)}&group=grp_&sday={str(date.day)}&archive={subject}&method=with&syear={str(date.year)}\"\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old does not work fully, misses articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "End of working code",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m End of working code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ethan\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit('End of working code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import datetime as dt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Get the current date and time\n",
    "now = dt.datetime.now()\n",
    "\n",
    "week_ago = now - dt.timedelta(days=3)\n",
    "articles = []\n",
    "url = f'http://export.arxiv.org/api/query?search_query=all'\n",
    "\n",
    "params = {\n",
    "    \"start\": 0, \n",
    "    \"max_results\": 100,  \n",
    "    \"sortBy\": \"submittedDate\",\n",
    "    \"sortOrder\": \"descending\",\n",
    "    'set':'cs'\n",
    "}\n",
    "i=0\n",
    "run_again = True\n",
    "while run_again:\n",
    "    print(params['start'])\n",
    "    resp = requests.get(url, params=params)\n",
    "    # Parse the XML response\n",
    "    ns = {'r': 'http://www.w3.org/2005/Atom'}\n",
    "    root = ET.fromstring(resp.text)\n",
    "\n",
    "    # Extract the article data\n",
    "\n",
    "    for entry in root.findall('r:entry', namespaces=ns):\n",
    "        i+=1\n",
    "        authors = []\n",
    "        for author in entry.findall('r:author', namespaces=ns):\n",
    "            authors.append(author.find('r:name', namespaces=ns).text)\n",
    "        paper_name = entry.find('r:title', namespaces=ns).text\n",
    "        abstract = entry.find('r:summary', namespaces=ns).text\n",
    "        date_published = entry.find('r:published', namespaces=ns).text\n",
    "        \n",
    "        if datetime.strptime(date_published[:10], \"%Y-%m-%d\") <= week_ago:\n",
    "            print(date_published)\n",
    "            run_again=False\n",
    "            break\n",
    "        affiliations = []\n",
    "        link = entry.find('r:link', namespaces=ns).get('href')\n",
    "        # if 'arxiv.org' in link:\n",
    "        #     print('https://arxiv.org/pdf/'+link.split('/')[-1]+'.pdf')\n",
    "        #     affiliations = download_arxiv('https://arxiv.org/pdf/'+link.split('/')[-1]+'.pdf')\n",
    "        article = {\n",
    "            'authors': authors,\n",
    "            'title': paper_name,\n",
    "            'abstract': abstract,\n",
    "            'date_published': date_published,\n",
    "            'link': link,\n",
    "            'affiliations':affiliations\n",
    "        }\n",
    "        articles.append(article)\n",
    "    params['start']+=params['max_results']\n",
    " \n",
    "df = pd.DataFrame.from_records(articles)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pdf_reader.pages[0].extract_text()\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'All' in None:\n",
    "    print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_affiliations_and_emails(page):\n",
    "    # Define the regex pattern for email addresses\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,6}\\b'\n",
    "    \n",
    "    # Find all email addresses in the page using re.findall\n",
    "    emails = re.findall(email_pattern, page)\n",
    "    \n",
    "    # Define a regex pattern to match affiliations\n",
    "    affiliation_pattern = r'(?:\\w+\\s){0,6}\\b(?:hospital|university|institute|school|academy|department|uc|tech)\\b(?:\\s\\w+){0,6}'\n",
    "\n",
    "    \n",
    "    # Find all affiliation matches in the page using re.findall\n",
    "    affiliations = re.findall(affiliation_pattern, page, re.IGNORECASE)\n",
    "    \n",
    "    # Remove duplicates by converting to a set and back to a list\n",
    "    output_list = list(set(emails + affiliations))\n",
    "    \n",
    "    return output_list\n",
    "\n",
    "\n",
    "\n",
    "page = \"\"\"\n",
    "Here are some emails: john.doe@example.com, alice.smith@example.org\n",
    "This is a t website. UC berkley\n",
    "Contact us: contact@university.edu\n",
    "\"\"\"\n",
    "emails = get_affiliations_and_emails(page)\n",
    "print(emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "page = \"\"\"Here are some emails: john.doe@example.com, alice.smith@example.org This is a t website. UC berkley Contact us: contact@university.edu\"\"\"\n",
    "\n",
    "pattern = r'((?:\\S+\\s*){0,4})(hospital|university|institute|school|academy|department|uc|tech)((?:\\s*\\S+){0,4})'\n",
    "matches = re.findall(pattern, page.replace('\\n', ' '), re.IGNORECASE)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "page = \"\"\"Here are some emails: john.doe@example.com, alice.smith@example.org This is a t website. UC berkley Contact us: contact@university.edu\"\"\"\n",
    "\n",
    "pattern = r'((?:\\S+\\s*){0,4})(hospital|university|institute|school|academy|department|uc|tech)((?:\\s*\\S+){0,4})'\n",
    "matches = re.findall(pattern, page, re.IGNORECASE)\n",
    "out = []\n",
    "for match in matches:\n",
    "    before, keyword1, after = match\n",
    "    full_match = f\"{before.strip()} {keyword1} {after.strip()}\".strip()\n",
    "    out.append(full_match)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Sample text string\n",
    "text_string = \"I studied at the London School of Economics and later taught at Harvard University.\"\n",
    "\n",
    "# Regex pattern matching\n",
    "pattern = r'\\b(at|from|in)?\\s*([\\w\\s]*?(University|College|Institute|Academy|School)[\\w\\s]*?)\\b'\n",
    "regex_matches = re.findall(pattern, text_string, re.IGNORECASE)\n",
    "regex_universities = [match[1] for match in regex_matches]\n",
    "\n",
    "# NER using spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text_string)\n",
    "ner_universities = [ent.text for ent in doc.ents if ent.label_ == 'ORG' and any(keyword in ent.text for keyword in ['University', 'College', 'Institute', 'Academy', 'School'])]\n",
    "\n",
    "# Combine results\n",
    "final_universities = list(set(regex_universities) | set(ner_universities))\n",
    "print(final_universities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text string\n",
    "text_string = \"Students at UCLA, University of Kansas, and Ohio State have done excellent research Emory.\"\n",
    "\n",
    "# Regex pattern matching\n",
    "def get_affiliations_and_emails(page):\n",
    "    page = page.replace('\\n', ' ').replace(',', ' ').replace('.com', '.com ')\n",
    "    page = page.translate(str.maketrans('', '', digits))\n",
    "    \n",
    "    pattern = r'\\b(at|from|in)?\\s*([\\w\\s]*?(University\\s+of\\s+\\w+|U\\.?[\\s]?[A-Z][\\.]?[\\s]?[A-Z]?[\\.]?[A-Z]?|U[A-Z]+|College|Institute|Academy|School|State|Tech|UC\\s+\\w+)[\\w\\s]*?)\\b'\n",
    "    regex_matches = re.findall(pattern, page, re.IGNORECASE)\n",
    "    regex_universities = [match[1].strip() for match in regex_matches]\n",
    "    doc = nlp(page)\n",
    "    ner_universities = [ent.text.strip() for ent in doc.ents if ent.label_ == 'ORG' and any(keyword in ent.text for keyword in ['University', 'College', 'Institute', 'Academy', 'School', 'MIT', 'Tech', 'State'])]\n",
    "    well_known_outliers = ['MIT', 'Caltech', 'Georgia Tech', 'UVA', \"Emory\"]  # Add more as needed\n",
    "    present_outliers = [outlier for outlier in well_known_outliers if outlier.lower() in page.lower()]\n",
    "    return list(set(regex_universities) | set(ner_universities) | set(present_outliers))\n",
    "#%timeit \n",
    "get_affiliations_and_emails(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affiliations_and_emails(page):\n",
    "    page = page.lower()  # Convert the page to lowercase for case-insensitive matching\n",
    "    page = page.replace('\\n', ' ').replace(',', ' ').replace('.com', '.com ')\n",
    "\n",
    "    pattern = r'\\b(at|from|in)?\\s*([\\w\\s]*?(university\\s+of\\s+\\w+|u\\.?[a-z]?[\\s]?[a-z]?[\\s]?[a-z]?|u[a-z]+|college|institute|academy|school|state|tech|uc\\s+\\w+)[\\w\\s]*?)\\b'\n",
    "    regex_matches = re.findall(pattern, page)\n",
    "    regex_universities = set(match[1].strip() for match in regex_matches)\n",
    "\n",
    "    doc = nlp(page)\n",
    "    ner_universities = set(ent.text.strip() for ent in doc.ents if ent.label_ == 'ORG' and any(keyword in ent.text.lower() for keyword in ['university', 'college', 'institute', 'academy', 'school', 'mit', 'tech', 'state']))\n",
    "\n",
    "    well_known_outliers = {'mit', 'caltech', 'georgia tech', 'uva', 'emory'}  # Use a set for faster membership testing\n",
    "    present_outliers = [outlier for outlier in well_known_outliers if outlier in page]\n",
    "\n",
    "    return list(regex_universities | ner_universities | set(present_outliers))\n",
    "\n",
    "# Example usage:\n",
    "text_string = \"Students at UCLA, University of Kansas, and Ohio State have done excellent research Emory.\"\n",
    "%timeit get_affiliations_and_emails(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affiliations_and_emails(page):\n",
    "    page = page.lower()  # Convert the page to lowercase for case-insensitive matching\n",
    "    page = page.replace('\\n', ' ').replace(',', ' ').replace('.com', '.com ')\n",
    "\n",
    "    pattern = r'\\b(at|from|in)?\\s*([\\w\\s]*?(university\\s+of\\s+\\w+|u\\.?[a-z]?[\\s]?[a-z]?[\\s]?[a-z]?|u[a-z]+|college|institute|academy|school|state|tech|uc\\s+\\w+)[\\w\\s]*?)\\b'\n",
    "    regex_matches = re.findall(pattern, page)\n",
    "    regex_universities = set(match[1].strip() for match in regex_matches)\n",
    "\n",
    "    doc = nlp(page)\n",
    "    ner_universities = set(ent.text.strip() for ent in doc.ents if ent.label_ == 'ORG' and any(keyword in ent.text.lower() for keyword in ['university', 'college', 'institute', 'academy', 'school', 'mit', 'tech', 'state']))\n",
    "\n",
    "    well_known_outliers = {'mit', 'caltech', 'georgia tech', 'uva', 'emory'}  # Use a set for faster membership testing\n",
    "    present_outliers = [outlier for outlier in well_known_outliers if outlier in page]\n",
    "\n",
    "    return list(regex_universities | ner_universities | set(present_outliers))\n",
    "\n",
    "# Example usage:\n",
    "text_string = \"Students at UCLA, University of Kansas, and Ohio State have done excellent research Emory.\"\n",
    "#%timeit \n",
    "get_affiliations_and_emails(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1075\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1076\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mdo_handshake()\n\u001b[0;32m   1347\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[0;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\packages\\six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    430\u001b[0m )\n\u001b[0;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    518\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    519\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    520\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    521\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    522\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    523\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    524\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    525\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1075\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1075\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1076\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1346\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mdo_handshake()\n\u001b[0;32m   1347\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\Documents\\ResearchFinder\\research_finder.ipynb Cell 37\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     patent_applications \u001b[39m=\u001b[39m fetch_patent_applications()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m patent_applications:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mfor\u001b[39;00m patent \u001b[39min\u001b[39;00m patent_applications:\n",
      "\u001b[1;32mc:\\Users\\ethan\\Documents\\ResearchFinder\\research_finder.ipynb Cell 37\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch_patent_applications\u001b[39m():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mbase_url\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mquery_params)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:547\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 547\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    549\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    551\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='api.grants.gov', port=443): Max retries exceeded with url: /v1/public/opportunities?page=1&rows=10&sortBy=openDate (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001BC5A69F3D0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11002] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001BC5A69F3D0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.grants.gov', port=443): Max retries exceeded with url: /v1/public/opportunities?page=1&rows=10&sortBy=openDate (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001BC5A69F3D0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ethan\\Documents\\ResearchFinder\\research_finder.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,           \u001b[39m# Page number\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrows\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m10\u001b[39m,          \u001b[39m# Number of rows per page\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msortBy\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mopenDate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# Sorting criteria (you can change this)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Make the API request\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(api_url, params\u001b[39m=\u001b[39;49mparams)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Check if the request was successful (status code 200)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ethan/Documents/ResearchFinder/research_finder.ipynb#X55sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\ethan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    562\u001b[0m         \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.grants.gov', port=443): Max retries exceeded with url: /v1/public/opportunities?page=1&rows=10&sortBy=openDate (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001BC5A69F3D0>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint URL\n",
    "api_url = \"https://api.grants.gov/v1/public/opportunities\"\n",
    "\n",
    "# Set up query parameters (you may need to adjust these)\n",
    "params = {\n",
    "    \"page\": 1,           # Page number\n",
    "    \"rows\": 10,          # Number of rows per page\n",
    "    \"sortBy\": \"openDate\" # Sorting criteria (you can change this)\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Parse the JSON response\n",
    "    # Now you can work with the data as needed\n",
    "    for opportunity in data[\"opportunityDetails\"]:\n",
    "        print(f\"Title: {opportunity['title']}\")\n",
    "        print(f\"Agency: {opportunity['agency']['name']}\")\n",
    "        print(f\"Posted Date: {opportunity['postedDate']}\")\n",
    "        print(f\"Application Due Date: {opportunity['closingDate']}\")\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
